{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e51c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queries import*\n",
    "from utilities import*\n",
    "from scipy.spatial import Delaunay\n",
    "from numpy import linalg\n",
    "from sklearn import preprocessing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc588252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treating patient3P1\n",
      "Importing edges...\n",
      "DONE\n",
      "Counting edges per phenotype...\n",
      "DONE\n",
      "Computing AUC...\n",
      "DONE\n",
      "Computing TILs...\n",
      "DONE\n",
      "Cell degree...\n",
      "DONE\n",
      "Building the whole graph...\n",
      "DONE\n",
      "Building the border graph...\n",
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 1436398\n",
      "Number of edges: 6090851\n",
      "Average degree:   8.4807\n",
      "Graph is not connected\n",
      "Diameter and Avg shortest path length are not defined!\n",
      "Sparsity: 0.0000\n",
      "Global clustering coefficient aka Transitivity: 0.5199\n",
      "DONE\n",
      "Connected components...\n",
      "The cell graph contains 1138 connected components\n",
      "The largest component has 1315814 nodes accounting for 91.61 % of the nodes\n",
      "231  components have at least  100  nodes\n"
     ]
    }
   ],
   "source": [
    "Directory = os.getcwd()\n",
    "Directory_path = Directory + '/data/input'\n",
    "files = os.listdir(Directory_path)\n",
    "plot_Directory = Directory + '/plots'\n",
    "\n",
    "for file in files[0:1]:\n",
    "    \n",
    "    array = file.split('_')\n",
    "    if len(array)==2:\n",
    "        condition = array[0]\n",
    "        PATIENT_ID = array[1]\n",
    "    else:\n",
    "        condition == ''\n",
    "        \n",
    "    \n",
    "    if condition == 'DEEPMEL':\n",
    "        \n",
    "        \n",
    "        print('Treating patient' + PATIENT_ID)\n",
    "\n",
    "        \n",
    "        INPUT_PATH = './data/input/DEEPMEL_' + PATIENT_ID + '/'\n",
    "        OUTPUT_PATH = './data/output/DEEPMEL_' + PATIENT_ID +'/'\n",
    "        PLOT_PATH ='./plots/DEEPMEL_' + PATIENT_ID +'/'\n",
    "        INPUT_FILE = 'DEEPMEL_'+ PATIENT_ID +'_cell_seg_data.csv'\n",
    "        TABLE_NAME_BQ = 'DEEPMEL_'+ PATIENT_ID +'_cell_seg_data'\n",
    "        SHOW_PLOTS =False   \n",
    "        HIGH_VOLUME = True\n",
    "        MIN_ELEMENTS_PER_CLUSTER = 100\n",
    "        ### The border values will extract only the border degined in the edge_neighberhood\n",
    "        BORDER = True\n",
    "        \n",
    "        \n",
    "        nodes = pd.read_csv(OUTPUT_PATH+'nodes_with_border.csv')\n",
    "        size_node_df =len(nodes)\n",
    "      \n",
    "                    \n",
    "        ### Setting cell_id as index\n",
    "\n",
    "        nodes.set_index('cell_id', inplace=True)\n",
    "        #nodes['phenotype'] = nodes['phenotype'].apply(lambda x: get_phenotype(x))\n",
    "        nodes_stats = pd.DataFrame(nodes.phenotype.value_counts()/len(nodes)).reset_index()\n",
    "        nodes_stats = pd.pivot_table(nodes_stats,columns='index').reset_index()\n",
    "        nodes_stats.columns.name = None \n",
    "        nodes_stats = nodes_stats.rename(columns={'index':'name'})\n",
    "        nodes_stats['name'] = PATIENT_ID\n",
    "        \n",
    "        \n",
    "        print('Importing edges...')\n",
    "        edges = pd.read_csv(OUTPUT_PATH + 'graph_edges.csv')\n",
    "        print('DONE')\n",
    "        cells_in_edges = get_cells_from_edges(edges)\n",
    "        \n",
    "        print('Counting edges per phenotype...')\n",
    "        N_EDGES = len(edges)\n",
    "        nodes_stats['nbr_edges'] = N_EDGES\n",
    "        edge_phenotype_count = edges.groupby(['phenotype_1','phenotype_2']).agg('count')[['cell_id_1']].reset_index().rename(columns = {'cell_id_1':'edge_count'})\n",
    "        edge_phenotype_count['edge_type'] = edge_phenotype_count.apply(lambda row : order_phenotypes(row.phenotype_1,row.phenotype_2), axis=1)\n",
    "        edge_phenotype_count = edge_phenotype_count[['edge_count','edge_type']]\n",
    "        edge_count = copy.deepcopy(edge_phenotype_count)\n",
    "        edge_count['edge_count'] = edge_phenotype_count['edge_count'].apply(lambda x: x/N_EDGES)\n",
    "        edge_phenotype_count_pivotted = pd.pivot_table(edge_count, values='edge_count',columns=['edge_type'], aggfunc=np.sum)\n",
    "        edge_phenotype_count_pivotted.columns.name = None\n",
    "        edge_phenotype_count_pivotted = edge_phenotype_count_pivotted.reset_index()\n",
    "        edge_phenotype_count_pivotted = edge_phenotype_count_pivotted.fillna(0)\n",
    "        edge_phenotype_count_pivotted = edge_phenotype_count_pivotted.drop(['index'], axis=1)\n",
    "        nodes_stats = nodes_stats.join(edge_phenotype_count_pivotted)\n",
    "        del edge_phenotype_count_pivotted\n",
    "\n",
    "        print('DONE')\n",
    "        print('Computing AUC...')\n",
    "        Min_distance = np.min(edges.distance)\n",
    "        Max_distance = np.max(edges.distance)\n",
    "        distribution = G_cross_function(edges,Min_distance,Max_distance,1000)\n",
    "        nodes_stats['AUC_total_graph'] = compute_AUC(distribution)\n",
    "        print('DONE')\n",
    "\n",
    "        print('Computing TILs...')\n",
    "\n",
    "        TILs = ['T','B','NK']\n",
    "        df_in_border_pivotted = get_infiltrated_tils(nodes, TILs, on_border=True)\n",
    "        df_in_border_pivotted = df_in_border_pivotted.rename(columns = {'B':'B_in','NK':'NK_in','T':'T_in'})\n",
    "        df_out_border_pivotted = get_infiltrated_tils(nodes, TILs, on_border=False)\n",
    "        df_out_border_pivotted = df_out_border_pivotted.rename(columns = {'B':'B_out','NK':'NK_out','T':'T_out'})\n",
    "        nodes_stats = nodes_stats.join(df_out_border_pivotted)\n",
    "        nodes_stats = nodes_stats.join(df_in_border_pivotted)\n",
    "        del df_out_border_pivotted\n",
    "        del df_in_border_pivotted\n",
    "        print('DONE')\n",
    "        \n",
    "        print('Cell degree...')\n",
    "        \n",
    "        cell_degree = pd.read_csv(OUTPUT_PATH+'cell_degree.csv')\n",
    "        \n",
    "        average_degree = pd.DataFrame(cell_degree.mean()).reset_index()\n",
    "        average_degree = pd.pivot_table(average_degree,columns='index').reset_index()\n",
    "        average_degree.columns.name =None \n",
    "        average_degree = average_degree.drop(['index','cell_id'], axis=1)\n",
    "        #nodes_stats = nodes_stats.rename(columns={'index':'name'})\n",
    "        nodes_stats = nodes_stats.join(average_degree)\n",
    "        nodes = nodes.merge(cell_degree, on='cell_id',how='left')\n",
    "        print('DONE')\n",
    "        \n",
    "        print('Building the whole graph...')\n",
    "        whole_cell_graph =nx.from_pandas_edgelist(edges, 'cell_id_1', 'cell_id_2', edge_attr=None, create_using= nx.Graph())\n",
    "        nodes_stats['whole_graph_sparsity'] = nx.density(whole_cell_graph)\n",
    "        nodes_stats['whole_graph_transitivity'] = nx.transitivity(whole_cell_graph)\n",
    "        del whole_cell_graph\n",
    "        print('DONE')\n",
    "\n",
    "        print('Building the border graph...')\n",
    "\n",
    "        \n",
    "        if BORDER:\n",
    "            edges = edges[edges.is_border== True]\n",
    "            nodes = nodes[nodes.on_border == True]\n",
    "    \n",
    "        cell_graph =nx.from_pandas_edgelist(edges, 'cell_id_1', 'cell_id_2', edge_attr=None, create_using= nx.Graph())\n",
    "        describe_graph(cell_graph)\n",
    "        nodes_stats['border_graph_sparsity'] = nx.density(cell_graph)\n",
    "        nodes_stats['border_graph_transitivity'] = nx.transitivity(cell_graph)\n",
    "        \n",
    "        plot_degree_distribution(cell_graph,savefig = True, figname = 'Degreed_itribution_bipartite.png',PLOT_PATH=PLOT_PATH,no_show = SHOW_PLOTS );\n",
    "        plt.close()\n",
    "\n",
    "        nx.set_node_attributes(cell_graph, nodes['cell_x_position'].to_dict(), 'x_position' )\n",
    "        nx.set_node_attributes(cell_graph, nodes['cell_y_position'].to_dict(), 'y_position' )\n",
    "        nx.set_node_attributes(cell_graph, nodes['tissue_category'].to_dict(), 'tissue_category' )\n",
    "        nx.set_node_attributes(cell_graph, nodes['phenotype'].to_dict(), 'phenotype' )\n",
    "        nx.set_node_attributes(cell_graph, nodes['on_border'].to_dict(), 'border')\n",
    "        print('DONE')\n",
    "        \n",
    "        \n",
    "        print('Connected components...')\n",
    "\n",
    "        components = list(nx.connected_components(cell_graph))\n",
    "        nodes_stats['n_initial_components'] =  len(components)\n",
    "        print('The cell graph contains', len(components), 'connected components')\n",
    "        largest_comp = max(components, key=len)\n",
    "        percentage_lcc = len(largest_comp)/cell_graph.number_of_nodes() * 100\n",
    "        nodes_stats['size_largest_component'] =  len(components)\n",
    "\n",
    "        print('The largest component has', len(largest_comp), 'nodes', 'accounting for %.2f'% percentage_lcc, '% of the nodes')\n",
    "        components = filtering_components(components, MIN_ELEMENTS_PER_CLUSTER)\n",
    "        components = sorted(components, key=len, reverse=False)\n",
    "        print(len(components),' components have at least ',MIN_ELEMENTS_PER_CLUSTER,' nodes')\n",
    "        nodes_stats['n_final_components'] =  len(components)\n",
    "        \n",
    "        \n",
    "        clustering_coef = nx.clustering(cell_graph)\n",
    "        clustering_df =  pd.DataFrame(clustering_coef.items(),columns=['cell_id','clustering_coef'])\n",
    "        nodes = nodes.merge(clustering_df, on ='cell_id', how='left')\n",
    "        nodes_stats['median_clustering_coefficient'] = np.median(clustering_df.clustering_coef.values)\n",
    "        del clustering_df\n",
    "        \n",
    "        components = list(nx.connected_components(cell_graph))\n",
    "        subgraphs = [cell_graph.subgraph(c).copy() for c in components]\n",
    "        \n",
    "        map_to_component = {}\n",
    "        for index, component in enumerate(components) :\n",
    "            elements_component = list(component)\n",
    "            for element in elements_component:\n",
    "                map_to_component[element] = index   \n",
    "                \n",
    "                \n",
    "        nodes['component'] = nodes.reset_index()['cell_id'].apply(lambda x: map_to_component[x]if x in map_to_component.keys() else '') \n",
    "        edges['component'] = edges.cell_id_1.apply(lambda x: map_to_component[x] if x in map_to_component.keys() else '')\n",
    "        \n",
    "        \n",
    "        size_df = nodes.groupby('component').agg('count').sort_values('cell_id')['cell_id'].reset_index().rename(columns = {'cell_id':'size'})\n",
    "        nodes = nodes.merge(size_df,on='component',how='left')\n",
    "        N = len(edges)\n",
    "        r = int((N/1000)/2000)+1 \n",
    "        q = [int(c) for c in np.linspace(0, N, r+2)]\n",
    "        edges_merged = pd.DataFrame({})\n",
    "        print('Dividing to '+ str(len(q)) + ' parts, done')\n",
    "        for index in range(len(q))[1:]:\n",
    "            lower_bound = q[index-1]\n",
    "            upper_bound = q[index]\n",
    "            edges_slice = edges[lower_bound:upper_bound].copy()\n",
    "            edges_slice = edges_slice.merge(size_df,on='component',how='left')\n",
    "            edges_merged = pd.concat([edges_merged,edges_slice])\n",
    "    \n",
    "        edges = edges_merged \n",
    "        del edges_merged\n",
    "        \n",
    "        print('Filtering components...')\n",
    "        \n",
    "        nodes = nodes[nodes['size']> MIN_ELEMENTS_PER_CLUSTER]\n",
    "        edges = edges[edges['size']> MIN_ELEMENTS_PER_CLUSTER]\n",
    "        print('DONE')\n",
    "\n",
    "        print('Computing AUC per component...')\n",
    "\n",
    "        AUC_per_component={}\n",
    "        components_in_graph = edges.component.unique()\n",
    "        for component in components_in_graph :\n",
    "    \n",
    "            edge_component_df = edges[edges.component == component].copy()\n",
    "            distribution =  G_cross_function(edge_component_df,min_radius=10, max_radius=20,radius_numbers=20,TILs=['T','B'])\n",
    "            area = compute_AUC(distribution)\n",
    "            AUC_per_component[component] = area\n",
    "    \n",
    "        AUC_per_component = pd.melt(pd.DataFrame(AUC_per_component, index = [0])) \n",
    "        AUC_per_component = AUC_per_component.rename(columns = {'variable':'component','value':'AUC_area'})  \n",
    "        \n",
    "        edges = edges.merge(AUC_per_component,on = 'component', how ='left')\n",
    "        nodes = nodes.merge(AUC_per_component,on = 'component', how ='left')\n",
    "        \n",
    "        print('DONE')\n",
    "        \n",
    "        \n",
    "        fig = AUC_per_component.sort_values(['AUC_area']).plot.bar(x='component',y='AUC_area',rot=0,figsize =(40,10))\n",
    "        plt.xlabel('Components')\n",
    "        plt.ylabel('AUC area')\n",
    "        plt.grid()\n",
    "        plt.title('AUC area per component')\n",
    "        plt.savefig(PLOT_PATH + 'AUC_area.png')\n",
    "        plt.close()\n",
    "        \n",
    "        del AUC_per_component\n",
    "\n",
    "        \n",
    "        \n",
    "        print('Count edges per component')\n",
    "        edge_phenotype_count = edges.groupby(['component','phenotype_1','phenotype_2']).agg('count')[['cell_id_1']].reset_index().rename(columns = {'cell_id_1':'edge_count'})\n",
    "        number_per_component = edges.groupby(['component']).agg('count')[['cell_id_1']].reset_index().rename(columns = {'cell_id_1':'total_edge_per_component'})\n",
    "        edge_phenotype_count = edge_phenotype_count.merge(number_per_component,on='component',how='left')\n",
    "        edge_phenotype_count['edge_count_ratio'] = edge_phenotype_count['edge_count']/edge_phenotype_count['total_edge_per_component']\n",
    "        edge_phenotype_count['edge_type'] = edge_phenotype_count.apply(lambda row : order_phenotypes(row.phenotype_1,row.phenotype_2), axis=1)\n",
    "        edge_phenotype_count = edge_phenotype_count[['component','edge_count_ratio','edge_type']]\n",
    "        \n",
    "        \n",
    "        edge_phenotype_count_pivotted = pd.pivot_table(edge_phenotype_count, values='edge_count_ratio', index=['component'],\n",
    "                    columns=['edge_type'], aggfunc=np.sum)\n",
    "\n",
    "        edge_phenotype_count_pivotted.columns.name = None\n",
    "        edge_phenotype_count_pivotted = edge_phenotype_count_pivotted.reset_index()\n",
    "        edge_phenotype_count_pivotted = edge_phenotype_count_pivotted.fillna(0)\n",
    "        print('DONE')\n",
    "        \n",
    "        columns_of_interest = edge_phenotype_count_pivotted.columns\n",
    "        columns = edge_phenotype_count_pivotted.columns\n",
    "        for interest in columns_of_interest:\n",
    "            if not(interest in columns):\n",
    "                edge_phenotype_count_pivotted[interest] = 0\n",
    "                \n",
    "        edges_of_interest = edge_phenotype_count_pivotted[columns_of_interest] \n",
    "        \n",
    "        plot_relplot(edges_of_interest,'component','tumor-T','Reds','The of T-tumor edges across components',True)\n",
    "        plt.close()\n",
    "\n",
    "        plot_relplot(edges_of_interest,'component','tumor-B','Reds','The of B-tumor edges across components',True)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        nodes = nodes.merge(edges_of_interest, on='component',how='left')\n",
    "        del edges_of_interest\n",
    "        del edge_phenotype_count_pivotted\n",
    "        del edge_phenotype_count\n",
    "        nodes = nodes.fillna(0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize = (15,15))\n",
    "        sns.scatterplot(data = nodes , x='cell_x_position', y= 'cell_y_position', hue = 'component').get_figure().savefig(PLOT_PATH+'scatter_component_250.png');\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        phenotype_component_grouped = pd.DataFrame(nodes.groupby(['component','phenotype']).agg('count')['cell_id']).reset_index()\n",
    "        phenotype_component_grouped = phenotype_component_grouped.merge(size_df, on='component',how='left')\n",
    "        phenotype_component_grouped['pheno_ratio'] = phenotype_component_grouped['cell_id']/phenotype_component_grouped['size']\n",
    "        phenotype_component_pivotted = pd.pivot_table(phenotype_component_grouped, values='pheno_ratio', index=['component'],\n",
    "                    columns=['phenotype'], aggfunc=np.sum)\n",
    "        phenotype_component_pivotted.columns.name = None\n",
    "        phenotype_component_pivotted = phenotype_component_pivotted.fillna(0).sort_values(['T']).reset_index()\n",
    "        \n",
    "        \n",
    "        plot_relplot(phenotype_component_pivotted,'component','tumor',PLOT_PATH,'Reds','The number of tumorous cells per component',True)\n",
    "        plt.close()\n",
    "\n",
    "        plot_relplot(phenotype_component_pivotted,'component','T',PLOT_PATH,'Reds','The number of T cells per component',True)\n",
    "        plt.close()\n",
    "\n",
    "        plot_relplot(phenotype_component_pivotted,'component','B',PLOT_PATH,'Reds','The number of B cells per component',True)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        phenotype_component_pivotted = phenotype_component_pivotted.merge(size_df, on='component',how='left')\n",
    "        phenotype_component_pivotted = phenotype_component_pivotted[phenotype_component_pivotted.tumor != 0 ].copy()\n",
    "        phenotype_component_pivotted = phenotype_component_pivotted.rename(columns = {'size_x':'size'})\n",
    "        \n",
    "        main_phenotypes = ['T','B','NK','stroma','tumor','dendtritic','macrophages']\n",
    "        columns = phenotype_component_pivotted.columns\n",
    "        for pheno in main_phenotypes:\n",
    "            if not(pheno in columns):\n",
    "                phenotype_component_pivotted[pheno] = 0\n",
    "                \n",
    "                \n",
    "        phenotype_component_pivotted['T_tumor_density'] = phenotype_component_pivotted['T']/phenotype_component_pivotted['tumor']\n",
    "        phenotype_component_pivotted['B_tumor_density'] = phenotype_component_pivotted['B']/phenotype_component_pivotted['tumor']\n",
    "        phenotype_component_pivotted['NK_tumor_density'] = phenotype_component_pivotted['NK']/phenotype_component_pivotted['tumor']\n",
    "        phenotype_component_pivotted['stroma_tumor_density'] = phenotype_component_pivotted['stroma']/phenotype_component_pivotted['tumor']\n",
    "        phenotype_component_pivotted['T_density'] = phenotype_component_pivotted['T']/phenotype_component_pivotted['size']\n",
    "        phenotype_component_pivotted['B_density'] = phenotype_component_pivotted['B']/phenotype_component_pivotted['size']\n",
    "        phenotype_component_pivotted['NK_density'] = phenotype_component_pivotted['NK']/phenotype_component_pivotted['size']\n",
    "        phenotype_component_pivotted['dendtritic_density'] = phenotype_component_pivotted['dendtritic']/phenotype_component_pivotted['size']\n",
    "        phenotype_component_pivotted['macrophages_density'] = phenotype_component_pivotted['macrophages']/phenotype_component_pivotted['size']   \n",
    "        \n",
    "        \n",
    "        plot_relplot(phenotype_component_pivotted,'component','T_tumor_density',PLOT_PATH,'Reds','T cells density over Tumor cells',True)\n",
    "        plt.close()\n",
    "\n",
    "        plot_relplot(phenotype_component_pivotted,'component','B_tumor_density',PLOT_PATH,'Reds','B cells density over Tumor cells',True)\n",
    "        plt.close()\n",
    "\n",
    "        plot_relplot(phenotype_component_pivotted,'component','NK_tumor_density',PLOT_PATH,'Reds','NK cells density over Tumor cells',True)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        nodes = nodes.merge(phenotype_component_pivotted, on='component', how='left')\n",
    "        \n",
    "        del phenotype_component_pivotted\n",
    "        del phenotype_component_grouped\n",
    "        \n",
    "        columns_to_avg = ['component','cell_x_position', 'cell_y_position', 'clustering_coef','missing_degree', 'stroma_degree', 'tumor_degree', 'total_degree',\n",
    "       'B_degree', 'MISSING_degree', 'NK_degree', 'T_degree',\n",
    "       'dendtritic_degree', 'macrophages_degree', 'tumor_pheno_degree', 'stroma_pheno_degree',\n",
    "       'T','B', 'MISSING', 'NK', 'dendtritic', 'macrophages', 'stroma', 'tumor',\n",
    "       'T_tumor_density', 'B_tumor_density', 'NK_tumor_density',\n",
    "       'stroma_tumor_density', 'T_density', 'B_density', 'NK_density',\n",
    "       'dendtritic_density', 'macrophages_density','AUC_area','B-B',\n",
    "       'MISSING-B', 'MISSING-MISSING', 'T-B', 'T-MISSING', 'T-T',\n",
    "       'dendtritic-B', 'dendtritic-MISSING', 'dendtritic-T',\n",
    "       'dendtritic-dendtritic', 'macrophages-B', 'macrophages-MISSING',\n",
    "       'macrophages-T', 'macrophages-dendtritic', 'macrophages-macrophages',\n",
    "       'stroma-B', 'stroma-MISSING', 'stroma-T', 'stroma-dendtritic',\n",
    "       'stroma-macrophages', 'stroma-stroma', 'tumor-B', 'tumor-MISSING',\n",
    "       'tumor-T', 'tumor-dendtritic', 'tumor-macrophages', 'tumor-stroma',\n",
    "       'tumor-tumor']\n",
    "\n",
    "        all_columns = nodes.columns\n",
    "        \n",
    "        nodes_to_components = add_null_column(nodes,columns_to_avg)\n",
    "        component_nodes = nodes_to_components.groupby(['component']).agg('mean').reset_index()\n",
    "        #component_nodes = component_nodes_sum.merge(component_nodes_avg, on='component', how ='left')\n",
    "        component_nodes = component_nodes.drop(['cell_id','on_border'],axis =1)\n",
    "        \n",
    "        component_nodes = nodes_to_components.groupby(['component']).agg('mean').reset_index()\n",
    "        #component_nodes = component_nodes_sum.merge(component_nodes_avg, on='component', how ='left')\n",
    "        component_nodes = component_nodes.drop(['cell_id','on_border'],axis =1)\n",
    "        \n",
    "        \n",
    "        component_x = component_nodes.cell_x_position.values\n",
    "        component_y = component_nodes.cell_y_position.values\n",
    "        points = []\n",
    "        for x,y in zip(component_x,component_y):\n",
    "            points.append([x,y])\n",
    "        points = np.array(points)    \n",
    "        tri = Delaunay(points)\n",
    "        \n",
    "        \n",
    "        fig = plt.figure(figsize=(15,7))\n",
    "        plt.triplot(points[:,0], points[:,1], tri.simplices)\n",
    "        plt.plot(points[:,0], points[:,1], 'o')\n",
    "        plt.title('Delaunay component triangularisation')    \n",
    "        plt.close()\n",
    "\n",
    "        fig.savefig(PLOT_PATH+'Delaunay_component_triangularisation.png')\n",
    "        \n",
    "        threshold = 1000\n",
    "        vertices = component_nodes.component.values\n",
    "        coord, edges_component = get_high_level_graph(component_nodes,threshold)\n",
    "        \n",
    "        \n",
    "        component_graph = nx.Graph() # for a directed graph use nx.DiGraph()\n",
    "        component_graph.add_nodes_from(range(len(vertices)))  # add multiple nodes at once\n",
    "        component_graph.add_edges_from(edges_component)\n",
    "        \n",
    "        \n",
    "        laplacian_matrix = nx.laplacian_matrix(component_graph).A\n",
    "        eigen_vals, eigen_vects = np.linalg.eig(laplacian_matrix)\n",
    "        idx = (-eigen_vals).argsort()[::-1]   \n",
    "        eigenValues = eigen_vals[idx]\n",
    "        eigenVectors = eigen_vects[:,idx]\n",
    "        \n",
    "        SIGNAL = np.array(component_nodes.AUC_area.fillna(0).values)\n",
    "        GRAPH = component_graph\n",
    "        FIGNAME = 'AUC area signal projection on the eigenvector space'\n",
    "        projection = project_signal(GRAPH,SIGNAL,PLOT_PATH,True,'b',True,FIGNAME)\n",
    "        plt.close() \n",
    "        \n",
    "        quadtratics = get_quadratic_laplacian_forms(component_nodes,vertices,edges_component)\n",
    "        quadtratics = pd.melt(quadtratics)\n",
    "        \n",
    "        fig = quadtratics.sort_values(['value']).plot.bar(x='variable',y='value',rot=70,figsize =(20,10))\n",
    "        plt.xlabel('Signals')\n",
    "        plt.ylabel('Quadratic Laplace form')\n",
    "        plt.grid()\n",
    "        plt.title('Quadratic Laplace form for all signals')\n",
    "        plt.savefig(PLOT_PATH + 'Quadratic Laplace form for all signals.png')\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        nodes.to_csv(OUTPUT_PATH + 'nodes_features.csv', index = False)\n",
    "        component_nodes.to_csv(OUTPUT_PATH + 'component_features.csv', index = False)\n",
    "        get_quadratic_laplacian_forms(component_nodes,vertices,edges_component).to_csv(OUTPUT_PATH + 'laplace_quadratic_form_high_level_graph.csv', index = False)\n",
    "        nodes_stats.to_csv(OUTPUT_PATH + 'nodes_stats.csv', index = False)\n",
    "        \n",
    "        del nodes\n",
    "        del edges\n",
    "        del nodes_stats\n",
    "        del component_nodes\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf66a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
